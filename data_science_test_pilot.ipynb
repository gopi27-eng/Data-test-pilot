{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d9e6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced Agentic Workflow Compiled!\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from typing import Annotated, TypedDict, List\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# 1. Define the Shared State (Memory)\n",
    "class AgentState(TypedDict):\n",
    "    # 'Annotated' with 'operator.add' ensures messages are appended, not overwritten\n",
    "    messages: Annotated[list[BaseMessage], operator.add]\n",
    "    project_idea: str\n",
    "    plan: str\n",
    "    review_feedback: str\n",
    "\n",
    "# 2. Initialize the Model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "# 3. Node 1: Planner Agent\n",
    "def planner_node(state: AgentState):\n",
    "    prompt = f\"As a Lead Data Scientist, create a test plan for: {state['project_idea']}\"\n",
    "    response = llm.invoke([SystemMessage(content=\"Create a detailed technical plan.\"), HumanMessage(content=prompt)])\n",
    "    return {\"plan\": response.content, \"messages\": [response]}\n",
    "\n",
    "# 4. Node 2: Critic Agent (The \"Reviewer\")\n",
    "def reviewer_node(state: AgentState):\n",
    "    prompt = f\"Review this plan for missing tools or unrealistic timelines: {state['plan']}\"\n",
    "    response = llm.invoke([SystemMessage(content=\"Be critical. Find 2 improvements.\"), HumanMessage(content=prompt)])\n",
    "    return {\"review_feedback\": response.content, \"messages\": [response]}\n",
    "\n",
    "# 5. Build the Graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"reviewer\", reviewer_node)\n",
    "\n",
    "# Define the flow: Start -> Planner -> Reviewer -> End\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"reviewer\")\n",
    "workflow.add_edge(\"reviewer\", END)\n",
    "\n",
    "# Compile the Agent\n",
    "app = workflow.compile()\n",
    "print(\"Advanced Agentic Workflow Compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ffedcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver # Temporary memory for the session\n",
    "\n",
    "# 1. Setup Memory (This allows the agent to 'wait' for you)\n",
    "memory = MemorySaver()\n",
    "\n",
    "# 2. Define a simple Approval Node\n",
    "def human_approval_node(state: AgentState):\n",
    "    # This node is a placeholder; the 'interrupt' happens before it\n",
    "    print(\"--- HUMAN APPROVAL GRANTED ---\")\n",
    "    return state\n",
    "\n",
    "# 3. Re-build the Graph with Interruption\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"reviewer\", reviewer_node)\n",
    "workflow.add_node(\"human_approval\", human_approval_node)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"reviewer\")\n",
    "workflow.add_edge(\"reviewer\", \"human_approval\")\n",
    "workflow.add_edge(\"human_approval\", END)\n",
    "\n",
    "# CRITICAL: We tell the graph to stop before 'human_approval'\n",
    "app = workflow.compile(checkpointer=memory, interrupt_before=[\"human_approval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fb5c06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'plan': 'As the Lead Data Scientist for the Airlines Cargo Weight Optimization project, this test plan outlines the strategy and specific tests required to ensure the robustness, accuracy, reliability, and business value of our Linear Regression model.\\n\\n---\\n\\n## Test Plan: Airlines Cargo Weight Optimization using Linear Regression\\n\\n**Document Version:** 1.0\\n**Date:** October 26, 2023\\n**Author:** [Your Name/Lead Data Scientist]\\n\\n---\\n\\n### 1. Introduction\\n\\nThis document details the test plan for the Airlines Cargo Weight Optimization model, which utilizes Linear Regression to predict optimal cargo weights for various flight segments, aircraft types, and operational conditions. The primary goal of this model is to maximize cargo revenue while adhering to critical safety and operational constraints (e.g., maximum takeoff weight, center of gravity limits, fuel efficiency).\\n\\nThe purpose of this test plan is to ensure that the developed model is:\\n*   **Accurate:** Provides predictions with acceptable error margins.\\n*   **Reliable:** Performs consistently across different data inputs and conditions.\\n*   **Robust:** Handles edge cases, missing data, and unexpected inputs gracefully.\\n*   **Compliant:** Adheres to all specified business rules, safety regulations, and operational constraints.\\n*   **Performant:** Delivers predictions within required latency thresholds.\\n*   **Valuable:** Drives tangible business improvements as intended.\\n\\n### 2. Scope\\n\\n#### 2.1 In Scope\\n*   **Data Ingestion & Feature Engineering:** Validation of data pipelines, transformations, and feature creation processes.\\n*   **Model Training & Evaluation:** Testing the training pipeline, hyperparameter tuning, model performance metrics, and overfitting/underfitting detection.\\n*   **Model Prediction/Inference:** Validation of model outputs, handling of new data, and performance under various load conditions.\\n*   **Model Robustness:** Testing behavior with edge cases, missing values, and corrupted inputs.\\n*   **Business Logic & Constraints:** Verification that the model\\'s recommendations respect airline-specific rules, safety limits (MTOW, CG), and operational feasibility.\\n*   **Model Versioning & Reproducibility:** Ensuring consistent results from saved models and reproducible training.\\n*   **Integration Testing (Model API):** Basic testing of the model\\'s API endpoint for receiving inputs and returning predictions.\\n\\n#### 2.2 Out of Scope\\n*   Full end-to-end system integration testing (e.g., integration with booking systems, ground operations software).\\n*   User Interface (UI) testing of any consuming applications.\\n*   Infrastructure and security testing of the deployment environment.\\n*   Performance testing of the entire microservice architecture (beyond the model\\'s inference speed).\\n\\n### 3. Assumptions\\n\\n*   **Data Availability & Quality:** Necessary historical flight data, cargo manifests, aircraft specifications, fuel data, and weather data are available and of sufficient quality for training and testing.\\n*   **Business Rules Defined:** All operational constraints, safety limits, and optimization objectives are clearly documented and understood.\\n*   **Stable Environment:** Testing environments (Dev, Staging) are stable and representative of production.\\n*   **Linear Relationship:** A sufficiently linear relationship exists between features and optimal cargo weight, justifying the use of Linear Regression.\\n*   **Domain Expertise:** Access to Subject Matter Experts (SMEs) from Flight Operations, Cargo Management, and Safety departments for validation.\\n*   **Infrastructure:** Necessary compute resources and MLOps tools (e.g., MLflow, DVC) are in place.\\n\\n### 4. Dependencies\\n\\n*   **Data Engineering Team:** For providing clean, transformed, and validated datasets.\\n*   **Platform/MLOps Team:** For setting up and maintaining the testing and deployment environments.\\n*   **Business Stakeholders/SMEs:** For clarifying requirements, providing ground truth, and validating business logic.\\n*   **Development Team:** For implementing the model, feature pipelines, and API endpoints.\\n\\n### 5. Test Environment\\n\\n*   **Development Environment:** Local machines for unit testing and initial feature development.\\n*   **Staging Environment:** A production-like environment for integration, performance, and user acceptance testing (UAT). This environment should mirror production data pipelines and infrastructure.\\n*   **Tools:** Python (Scikit-learn, Pandas, NumPy, Matplotlib, Seaborn), Jupyter Notebooks, Git, MLflow/DVC (for model tracking and versioning), containerization (Docker), cloud platform (AWS/Azure/GCP).\\n\\n### 6. Test Data\\n\\nA comprehensive set of test data is crucial for robust model validation.\\n\\n*   **Training Data:** Historically validated data used to train the model.\\n*   **Validation Data:** A separate subset of historical data used for hyperparameter tuning and initial performance checks during development.\\n*   **Holdout Test Data:** An entirely unseen dataset, representative of real-world scenarios, reserved solely for final model performance evaluation. This data should cover:\\n    *   **Normal Operations:** Typical flight routes, aircraft types, and cargo loads.\\n    *   **Edge Cases:**\\n        *   **Minimum/Maximum Cargo:** Flights with very low or very high cargo demand.\\n        *   **Specific Aircrafts:** All aircraft types in the fleet, including new or less common ones.\\n        *   **Route Variations:** Short-haul, long-haul, domestic, international, high-altitude airports.\\n        *   **Seasonal Variations:** Data representing different seasons (e.g., peak holiday shipping vs. off-peak).\\n        *   **Weather Extremes:** Data simulating adverse weather conditions impacting fuel/weight.\\n        *   **Empty Flights:** Flights with no cargo.\\n        *   **Full Flights:** Flights where cargo capacity is nearly maximized.\\n    *   **Data with Missing Values:** To test imputation strategies.\\n    *   **Outlier Data:** To test model robustness against anomalous inputs.\\n    *   **\"Adversarial\" Data:** Hand-crafted scenarios designed to challenge model assumptions or expose weaknesses.\\n*   **Synthetic Data:** Generated data to simulate specific scenarios not abundantly present in historical data (e.g., new routes, new aircraft configurations).\\n*   **Production Data Subset (for Shadow/A/B Testing):** A small, real-time stream of production data to compare model predictions against current operational decisions or a baseline model post-deployment.\\n\\n### 7. Test Cases\\n\\nTest cases are categorized to ensure comprehensive coverage.\\n\\n#### 7.1 Unit Tests (Developer-led)\\n*   **Feature Engineering Functions:**\\n    *   Test individual feature transformations (e.g., one-hot encoding, scaling, interaction terms).\\n    *   Validate handling of missing values within specific features.\\n    *   Ensure correct data types and ranges for generated features.\\n*   **Data Loading & Preprocessing:**\\n    *   Verify data ingestion from source systems.\\n    *   Test data cleaning routines (e.g., outlier detection, imputation).\\n*   **Model Utility Functions:**\\n    *   Test functions for model saving/loading (serialization/deserialization).\\n    *   Test custom evaluation metrics.\\n\\n#### 7.2 Data Validation Tests (Automated & Manual)\\n*   **Schema Validation:**\\n    *   Verify that input data conforms to the expected schema (column names, data types).\\n*   **Range & Constraint Validation:**\\n    *   Ensure numerical features (e.g., fuel weight, passenger count, distance) are within plausible and defined ranges.\\n    *   Validate categorical features against known allowed values (e.g., aircraft types, origin/destination airports).\\n*   **Missing Value Analysis:**\\n    *   Verify that missing values are handled as expected (imputation, dropping rows/features).\\n    *   Ensure no critical features have an excessively high proportion of missing data.\\n*   **Outlier Detection:**\\n    *   Verify outlier detection and handling mechanisms (e.g., capping, transformation) are effective and do not remove valid data points.\\n*   **Data Consistency:**\\n    *   Cross-check related data points (e.g., if total weight is calculated, ensure it sums correctly).\\n    *   Verify temporal consistency (e.g., no future dates in historical data).\\n\\n#### 7.3 Model Training & Evaluation Tests\\n*   **Training Pipeline Integrity:**\\n    *   Verify the training script runs to completion without errors.\\n    *   Ensure model artifacts (e.g., trained model, scaler, feature encoders) are saved correctly.\\n    *   Confirm model metadata (e.g., hyperparameters, training date, dataset version) is logged (e.g., in MLflow).\\n*   **Cross-Validation:**\\n    *   Verify the cross-validation strategy is correctly implemented and provides stable results.\\n*   **Performance Metrics:**\\n    *   Calculate and record key regression metrics on the holdout test set:\\n        *   **R-squared (R²):** Should meet a predefined threshold (e.g., > 0.85).\\n        *   **Mean Absolute Error (MAE):** Should be below a specific threshold (e.g., < 500 kg).\\n        *   **Root Mean Squared Error (RMSE):** Should be below a specific threshold (e.g., < 750 kg).\\n        *   **Mean Absolute Percentage Error (MAPE):** Should be below a specific threshold (e.g., < 5%).\\n    *   Compare metrics against a baseline model (e.g., current manual estimation, a simpler heuristic).\\n*   **Overfitting/Underfitting Detection:**\\n    *   Compare training set performance to validation/test set performance. Significant discrepancies indicate overfitting.\\n    *   Analyze learning curves.\\n*   **Residual Analysis (Specific to Linear Regression):**\\n    *   **Residuals vs. Fitted Values Plot:** Check for homoscedasticity (constant variance) and linearity (no patterns).\\n    *   **Normal Q-Q Plot of Residuals:** Verify normality assumption.\\n    *   **Histogram of Residuals:** Visually inspect for normality.\\n*   **Coefficient Analysis:**\\n    *   **Sign and Magnitude:** Verify that coefficients of key features (e.g., distance, passenger count, fuel weight) have expected signs and reasonable magnitudes based on domain knowledge.\\n    *   **P-values:** Ensure statistically significant features have low p-values.\\n*   **Feature Importance:**\\n    *   Verify that features considered important by domain experts are indeed influential in the model (e.g., using permutation importance or coefficient magnitudes).\\n*   **Reproducibility:**\\n    *   Retrain the model using the same code, data, and random seed; ensure identical model parameters and predictions.\\n\\n#### 7.4 Prediction/Inference Tests\\n*   **Output Format & Range:**\\n    *   Verify that predicted cargo weights are numerical, within plausible physical limits (e.g., non-negative, below aircraft max structural cargo capacity).\\n    *   Ensure the output format matches API specifications.\\n*   **Consistency:**\\n    *   Provide the same input multiple times; verify identical predictions.\\n*   **Edge Cases (using dedicated test data):**\\n    *   Test with minimum/maximum values for all input features.\\n    *   Test with new routes or aircraft types not seen in training (if model is designed to generalize).\\n    *   Test with zero cargo demand scenarios.\\n    *   Test with flights where current cargo estimates are near aircraft limits.\\n*   **Missing Input Handling:**\\n    *   Test how the model handles missing values in inference requests (e.g., returns error, uses default, imputes).\\n*   **Inference Latency:**\\n    *   Measure the time taken for a single prediction request. Ensure it meets the specified operational latency requirements (e.g., < 50ms).\\n\\n#### 7.5 Robustness & Stability Tests\\n*   **Data Drift Simulation:**\\n    *   Introduce synthetic data with slight shifts in feature distributions (e.g., a new average passenger weight) and observe prediction stability.\\n*   **Error Handling:**\\n    *   Provide malformed inputs (e.g., wrong data types, out-of-range values, invalid categorical values) to the prediction API and verify appropriate error responses.\\n*   **Model Degradation:**\\n    *   Simulate data with increasing levels of noise or missingness to understand model degradation tolerance.\\n*   **Retraining Process:**\\n    *   If an automated retraining pipeline exists, test its ability to retrain the model with new data and deploy the updated version without manual intervention.\\n\\n#### 7.6 Business Logic & Domain Specific Tests (Crucial for \"Optimization\")\\n*   **Safety Constraints Adherence:**\\n    *   **Maximum Takeoff Weight (MTOW):** Verify that the *recommended* total aircraft weight (empty weight + fuel + passengers + predicted cargo) never exceeds the MTOW for the specific aircraft and airport.\\n    *   **Center of Gravity (CG) Limits:** If the model optimizes cargo distribution, verify that the predicted distribution keeps the aircraft\\'s CG within safe operational envelopes. If the model only predicts total weight, ensure the total weight leaves sufficient margin for safe loading within CG limits.\\n    *   **Structural Cargo Limits:** Ensure predicted cargo weight does not exceed the structural capacity of the cargo holds.\\n*   **Operational Feasibility:**\\n    *   Are the predicted weights actionable by ground crew? (e.g., values are not too precise that they are impractical to load).\\n    *   Do recommendations align with typical cargo loading practices?\\n*   **Revenue Optimization Impact:**\\n    *   Compare the model\\'s predicted cargo capacity (and potential revenue) against current operational estimates or historical actuals. The model should demonstrate a potential uplift in cargo utilization/revenue.\\n    *   Analyze scenarios where the model enables carrying more cargo than current methods, without compromising safety.\\n*   **Conflict Resolution:**\\n    *   Test scenarios where maximizing cargo might conflict with other factors (e.g., fuel efficiency for very heavy loads). Ensure the model\\'s objective function balances these appropriately.\\n*   **Regulatory Compliance:**\\n    *   Verify that any aviation-specific regulations related to weight and balance are implicitly or explicitly handled by the model.\\n\\n#### 7.7 Integration Tests (API Level)\\n*   **API Endpoint Availability:**\\n    *   Confirm the prediction API endpoint is accessible and responsive.\\n*   **Request/Response Format:**\\n    *   Send sample valid requests and verify that the JSON/XML response format is correct and contains the expected data fields.\\n*   **Authentication/Authorization:**\\n    *   If applicable, test with valid and invalid credentials.\\n\\n### 8. Test Metrics & Pass/Fail Criteria\\n\\n*   **Quantitative Metrics (Model Performance):**\\n    *   R-squared (R²): >= [Target Threshold, e.g., 0.85]\\n    *   MAE: <= [Target Threshold, e.g., 500 kg]\\n    *   RMSE: <= [Target Threshold, e.g., 750 kg]\\n    *   MAPE: <= [Target Threshold, e.g., 5%]\\n    *   Inference Latency: <= [Target Threshold, e.g., 50 ms per prediction]\\n*   **Qualitative Metrics (Business & Robustness):**\\n    *   **Residual Plots:** Visually inspect for randomness, homoscedasticity, and normality.\\n    *   **Coefficient Signs:** Must align with domain expectations.\\n    *   **Safety Constraint Violations:** 0 violations in all tested scenarios.\\n    *   **Business Acceptance:** Model recommendations are deemed operationally feasible and beneficial by SMEs.\\n    *   **Error Handling:** All invalid inputs result in graceful error messages, not system crashes.\\n    *   **Data Drift:** Model performance degradation below a threshold (e.g., 10% increase in MAE) should trigger alerts.\\n\\n*   **Pass/Fail Criteria:**\\n    *   **PASS:** All specified quantitative metrics are met, all qualitative checks are satisfactory, and no critical defects are found.\\n    *   **FAIL:** Any quantitative metric fails to meet its threshold, critical business logic or safety constraints are violated, or major defects preventing model usage are identified.\\n\\n### 9. Roles & Responsibilities\\n\\n*   **Lead Data Scientist (You):** Overall test plan ownership, defining test strategy, reviewing test results, approving model for deployment.\\n*   **Data Scientists:** Develop unit tests, implement evaluation metrics, analyze model performance, conduct residual analysis, investigate model-related defects.\\n*   **Data Engineers:** Ensure data quality and availability, support data validation tests, investigate data pipeline-related defects.\\n*   **QA Engineer(s) (if applicable):** Execute integration tests, manage test cases, log defects, ensure test environment stability.\\n*   **Business Analysts/SMEs:** Provide input on business rules and constraints, validate model recommendations for operational feasibility and safety.\\n*   **Project Manager:** Facilitate communication, track progress, manage resources, approve timelines.\\n\\n### 10. Schedule & Timeline (High-Level)\\n\\n*   **Phase 1: Planning & Setup (1 week)**\\n    *   Define test data requirements, environment setup.\\n    *   Finalize test plan.\\n*   **Phase 2: Unit & Data Validation Testing (2 weeks)**\\n    *   Individual component testing, automated data quality checks.\\n*   **Phase 3: Model Training & Evaluation Testing (2 weeks)**\\n    *   Performance metric calculation, residual analysis, robustness testing.\\n*   **Phase 4: Business Logic & Integration Testing (2 weeks)**\\n    *   SME review, constraint validation, API testing.\\n*   **Phase 5: User Acceptance Testing (UAT) & Sign-off (1 week)**\\n    *   Final stakeholder review and approval.\\n\\n### 11. Reporting & Defect Management\\n\\n*   **Test Report:** A summary document will be generated at the end of each major testing phase, detailing test coverage, executed test cases, results, identified defects, and overall model readiness.\\n*   **Defect Tracking:** All identified issues, bugs, or discrepancies will be logged in a centralized defect tracking system (e.g., JIRA). Each defect will include:\\n    *   Description\\n    *   Steps to reproduce\\n    *   Expected vs. Actual results\\n    *   Severity (Critical, Major, Minor)\\n    *   Priority (High, Medium, Low)\\n    *   Assigned owner\\n*   **Communication:** Regular status meetings will be held with stakeholders to review progress, discuss blockers, and prioritize defect resolution.\\n\\n### 12. Sign-off\\n\\nBy signing below, the undersigned parties acknowledge their review and approval of this Test Plan.\\n\\n---\\n**Lead Data Scientist:**\\nName: [Your Name]\\nSignature: _________________________\\nDate: _________________________\\n\\n**Project Manager:**\\nName: [PM Name]\\nSignature: _________________________\\nDate: _________________________\\n\\n**Business Stakeholder / SME:**\\nName: [SME Name]\\nSignature: _________________________\\nDate: _________________________\\n\\n---', 'messages': [AIMessage(content='As the Lead Data Scientist for the Airlines Cargo Weight Optimization project, this test plan outlines the strategy and specific tests required to ensure the robustness, accuracy, reliability, and business value of our Linear Regression model.\\n\\n---\\n\\n## Test Plan: Airlines Cargo Weight Optimization using Linear Regression\\n\\n**Document Version:** 1.0\\n**Date:** October 26, 2023\\n**Author:** [Your Name/Lead Data Scientist]\\n\\n---\\n\\n### 1. Introduction\\n\\nThis document details the test plan for the Airlines Cargo Weight Optimization model, which utilizes Linear Regression to predict optimal cargo weights for various flight segments, aircraft types, and operational conditions. The primary goal of this model is to maximize cargo revenue while adhering to critical safety and operational constraints (e.g., maximum takeoff weight, center of gravity limits, fuel efficiency).\\n\\nThe purpose of this test plan is to ensure that the developed model is:\\n*   **Accurate:** Provides predictions with acceptable error margins.\\n*   **Reliable:** Performs consistently across different data inputs and conditions.\\n*   **Robust:** Handles edge cases, missing data, and unexpected inputs gracefully.\\n*   **Compliant:** Adheres to all specified business rules, safety regulations, and operational constraints.\\n*   **Performant:** Delivers predictions within required latency thresholds.\\n*   **Valuable:** Drives tangible business improvements as intended.\\n\\n### 2. Scope\\n\\n#### 2.1 In Scope\\n*   **Data Ingestion & Feature Engineering:** Validation of data pipelines, transformations, and feature creation processes.\\n*   **Model Training & Evaluation:** Testing the training pipeline, hyperparameter tuning, model performance metrics, and overfitting/underfitting detection.\\n*   **Model Prediction/Inference:** Validation of model outputs, handling of new data, and performance under various load conditions.\\n*   **Model Robustness:** Testing behavior with edge cases, missing values, and corrupted inputs.\\n*   **Business Logic & Constraints:** Verification that the model\\'s recommendations respect airline-specific rules, safety limits (MTOW, CG), and operational feasibility.\\n*   **Model Versioning & Reproducibility:** Ensuring consistent results from saved models and reproducible training.\\n*   **Integration Testing (Model API):** Basic testing of the model\\'s API endpoint for receiving inputs and returning predictions.\\n\\n#### 2.2 Out of Scope\\n*   Full end-to-end system integration testing (e.g., integration with booking systems, ground operations software).\\n*   User Interface (UI) testing of any consuming applications.\\n*   Infrastructure and security testing of the deployment environment.\\n*   Performance testing of the entire microservice architecture (beyond the model\\'s inference speed).\\n\\n### 3. Assumptions\\n\\n*   **Data Availability & Quality:** Necessary historical flight data, cargo manifests, aircraft specifications, fuel data, and weather data are available and of sufficient quality for training and testing.\\n*   **Business Rules Defined:** All operational constraints, safety limits, and optimization objectives are clearly documented and understood.\\n*   **Stable Environment:** Testing environments (Dev, Staging) are stable and representative of production.\\n*   **Linear Relationship:** A sufficiently linear relationship exists between features and optimal cargo weight, justifying the use of Linear Regression.\\n*   **Domain Expertise:** Access to Subject Matter Experts (SMEs) from Flight Operations, Cargo Management, and Safety departments for validation.\\n*   **Infrastructure:** Necessary compute resources and MLOps tools (e.g., MLflow, DVC) are in place.\\n\\n### 4. Dependencies\\n\\n*   **Data Engineering Team:** For providing clean, transformed, and validated datasets.\\n*   **Platform/MLOps Team:** For setting up and maintaining the testing and deployment environments.\\n*   **Business Stakeholders/SMEs:** For clarifying requirements, providing ground truth, and validating business logic.\\n*   **Development Team:** For implementing the model, feature pipelines, and API endpoints.\\n\\n### 5. Test Environment\\n\\n*   **Development Environment:** Local machines for unit testing and initial feature development.\\n*   **Staging Environment:** A production-like environment for integration, performance, and user acceptance testing (UAT). This environment should mirror production data pipelines and infrastructure.\\n*   **Tools:** Python (Scikit-learn, Pandas, NumPy, Matplotlib, Seaborn), Jupyter Notebooks, Git, MLflow/DVC (for model tracking and versioning), containerization (Docker), cloud platform (AWS/Azure/GCP).\\n\\n### 6. Test Data\\n\\nA comprehensive set of test data is crucial for robust model validation.\\n\\n*   **Training Data:** Historically validated data used to train the model.\\n*   **Validation Data:** A separate subset of historical data used for hyperparameter tuning and initial performance checks during development.\\n*   **Holdout Test Data:** An entirely unseen dataset, representative of real-world scenarios, reserved solely for final model performance evaluation. This data should cover:\\n    *   **Normal Operations:** Typical flight routes, aircraft types, and cargo loads.\\n    *   **Edge Cases:**\\n        *   **Minimum/Maximum Cargo:** Flights with very low or very high cargo demand.\\n        *   **Specific Aircrafts:** All aircraft types in the fleet, including new or less common ones.\\n        *   **Route Variations:** Short-haul, long-haul, domestic, international, high-altitude airports.\\n        *   **Seasonal Variations:** Data representing different seasons (e.g., peak holiday shipping vs. off-peak).\\n        *   **Weather Extremes:** Data simulating adverse weather conditions impacting fuel/weight.\\n        *   **Empty Flights:** Flights with no cargo.\\n        *   **Full Flights:** Flights where cargo capacity is nearly maximized.\\n    *   **Data with Missing Values:** To test imputation strategies.\\n    *   **Outlier Data:** To test model robustness against anomalous inputs.\\n    *   **\"Adversarial\" Data:** Hand-crafted scenarios designed to challenge model assumptions or expose weaknesses.\\n*   **Synthetic Data:** Generated data to simulate specific scenarios not abundantly present in historical data (e.g., new routes, new aircraft configurations).\\n*   **Production Data Subset (for Shadow/A/B Testing):** A small, real-time stream of production data to compare model predictions against current operational decisions or a baseline model post-deployment.\\n\\n### 7. Test Cases\\n\\nTest cases are categorized to ensure comprehensive coverage.\\n\\n#### 7.1 Unit Tests (Developer-led)\\n*   **Feature Engineering Functions:**\\n    *   Test individual feature transformations (e.g., one-hot encoding, scaling, interaction terms).\\n    *   Validate handling of missing values within specific features.\\n    *   Ensure correct data types and ranges for generated features.\\n*   **Data Loading & Preprocessing:**\\n    *   Verify data ingestion from source systems.\\n    *   Test data cleaning routines (e.g., outlier detection, imputation).\\n*   **Model Utility Functions:**\\n    *   Test functions for model saving/loading (serialization/deserialization).\\n    *   Test custom evaluation metrics.\\n\\n#### 7.2 Data Validation Tests (Automated & Manual)\\n*   **Schema Validation:**\\n    *   Verify that input data conforms to the expected schema (column names, data types).\\n*   **Range & Constraint Validation:**\\n    *   Ensure numerical features (e.g., fuel weight, passenger count, distance) are within plausible and defined ranges.\\n    *   Validate categorical features against known allowed values (e.g., aircraft types, origin/destination airports).\\n*   **Missing Value Analysis:**\\n    *   Verify that missing values are handled as expected (imputation, dropping rows/features).\\n    *   Ensure no critical features have an excessively high proportion of missing data.\\n*   **Outlier Detection:**\\n    *   Verify outlier detection and handling mechanisms (e.g., capping, transformation) are effective and do not remove valid data points.\\n*   **Data Consistency:**\\n    *   Cross-check related data points (e.g., if total weight is calculated, ensure it sums correctly).\\n    *   Verify temporal consistency (e.g., no future dates in historical data).\\n\\n#### 7.3 Model Training & Evaluation Tests\\n*   **Training Pipeline Integrity:**\\n    *   Verify the training script runs to completion without errors.\\n    *   Ensure model artifacts (e.g., trained model, scaler, feature encoders) are saved correctly.\\n    *   Confirm model metadata (e.g., hyperparameters, training date, dataset version) is logged (e.g., in MLflow).\\n*   **Cross-Validation:**\\n    *   Verify the cross-validation strategy is correctly implemented and provides stable results.\\n*   **Performance Metrics:**\\n    *   Calculate and record key regression metrics on the holdout test set:\\n        *   **R-squared (R²):** Should meet a predefined threshold (e.g., > 0.85).\\n        *   **Mean Absolute Error (MAE):** Should be below a specific threshold (e.g., < 500 kg).\\n        *   **Root Mean Squared Error (RMSE):** Should be below a specific threshold (e.g., < 750 kg).\\n        *   **Mean Absolute Percentage Error (MAPE):** Should be below a specific threshold (e.g., < 5%).\\n    *   Compare metrics against a baseline model (e.g., current manual estimation, a simpler heuristic).\\n*   **Overfitting/Underfitting Detection:**\\n    *   Compare training set performance to validation/test set performance. Significant discrepancies indicate overfitting.\\n    *   Analyze learning curves.\\n*   **Residual Analysis (Specific to Linear Regression):**\\n    *   **Residuals vs. Fitted Values Plot:** Check for homoscedasticity (constant variance) and linearity (no patterns).\\n    *   **Normal Q-Q Plot of Residuals:** Verify normality assumption.\\n    *   **Histogram of Residuals:** Visually inspect for normality.\\n*   **Coefficient Analysis:**\\n    *   **Sign and Magnitude:** Verify that coefficients of key features (e.g., distance, passenger count, fuel weight) have expected signs and reasonable magnitudes based on domain knowledge.\\n    *   **P-values:** Ensure statistically significant features have low p-values.\\n*   **Feature Importance:**\\n    *   Verify that features considered important by domain experts are indeed influential in the model (e.g., using permutation importance or coefficient magnitudes).\\n*   **Reproducibility:**\\n    *   Retrain the model using the same code, data, and random seed; ensure identical model parameters and predictions.\\n\\n#### 7.4 Prediction/Inference Tests\\n*   **Output Format & Range:**\\n    *   Verify that predicted cargo weights are numerical, within plausible physical limits (e.g., non-negative, below aircraft max structural cargo capacity).\\n    *   Ensure the output format matches API specifications.\\n*   **Consistency:**\\n    *   Provide the same input multiple times; verify identical predictions.\\n*   **Edge Cases (using dedicated test data):**\\n    *   Test with minimum/maximum values for all input features.\\n    *   Test with new routes or aircraft types not seen in training (if model is designed to generalize).\\n    *   Test with zero cargo demand scenarios.\\n    *   Test with flights where current cargo estimates are near aircraft limits.\\n*   **Missing Input Handling:**\\n    *   Test how the model handles missing values in inference requests (e.g., returns error, uses default, imputes).\\n*   **Inference Latency:**\\n    *   Measure the time taken for a single prediction request. Ensure it meets the specified operational latency requirements (e.g., < 50ms).\\n\\n#### 7.5 Robustness & Stability Tests\\n*   **Data Drift Simulation:**\\n    *   Introduce synthetic data with slight shifts in feature distributions (e.g., a new average passenger weight) and observe prediction stability.\\n*   **Error Handling:**\\n    *   Provide malformed inputs (e.g., wrong data types, out-of-range values, invalid categorical values) to the prediction API and verify appropriate error responses.\\n*   **Model Degradation:**\\n    *   Simulate data with increasing levels of noise or missingness to understand model degradation tolerance.\\n*   **Retraining Process:**\\n    *   If an automated retraining pipeline exists, test its ability to retrain the model with new data and deploy the updated version without manual intervention.\\n\\n#### 7.6 Business Logic & Domain Specific Tests (Crucial for \"Optimization\")\\n*   **Safety Constraints Adherence:**\\n    *   **Maximum Takeoff Weight (MTOW):** Verify that the *recommended* total aircraft weight (empty weight + fuel + passengers + predicted cargo) never exceeds the MTOW for the specific aircraft and airport.\\n    *   **Center of Gravity (CG) Limits:** If the model optimizes cargo distribution, verify that the predicted distribution keeps the aircraft\\'s CG within safe operational envelopes. If the model only predicts total weight, ensure the total weight leaves sufficient margin for safe loading within CG limits.\\n    *   **Structural Cargo Limits:** Ensure predicted cargo weight does not exceed the structural capacity of the cargo holds.\\n*   **Operational Feasibility:**\\n    *   Are the predicted weights actionable by ground crew? (e.g., values are not too precise that they are impractical to load).\\n    *   Do recommendations align with typical cargo loading practices?\\n*   **Revenue Optimization Impact:**\\n    *   Compare the model\\'s predicted cargo capacity (and potential revenue) against current operational estimates or historical actuals. The model should demonstrate a potential uplift in cargo utilization/revenue.\\n    *   Analyze scenarios where the model enables carrying more cargo than current methods, without compromising safety.\\n*   **Conflict Resolution:**\\n    *   Test scenarios where maximizing cargo might conflict with other factors (e.g., fuel efficiency for very heavy loads). Ensure the model\\'s objective function balances these appropriately.\\n*   **Regulatory Compliance:**\\n    *   Verify that any aviation-specific regulations related to weight and balance are implicitly or explicitly handled by the model.\\n\\n#### 7.7 Integration Tests (API Level)\\n*   **API Endpoint Availability:**\\n    *   Confirm the prediction API endpoint is accessible and responsive.\\n*   **Request/Response Format:**\\n    *   Send sample valid requests and verify that the JSON/XML response format is correct and contains the expected data fields.\\n*   **Authentication/Authorization:**\\n    *   If applicable, test with valid and invalid credentials.\\n\\n### 8. Test Metrics & Pass/Fail Criteria\\n\\n*   **Quantitative Metrics (Model Performance):**\\n    *   R-squared (R²): >= [Target Threshold, e.g., 0.85]\\n    *   MAE: <= [Target Threshold, e.g., 500 kg]\\n    *   RMSE: <= [Target Threshold, e.g., 750 kg]\\n    *   MAPE: <= [Target Threshold, e.g., 5%]\\n    *   Inference Latency: <= [Target Threshold, e.g., 50 ms per prediction]\\n*   **Qualitative Metrics (Business & Robustness):**\\n    *   **Residual Plots:** Visually inspect for randomness, homoscedasticity, and normality.\\n    *   **Coefficient Signs:** Must align with domain expectations.\\n    *   **Safety Constraint Violations:** 0 violations in all tested scenarios.\\n    *   **Business Acceptance:** Model recommendations are deemed operationally feasible and beneficial by SMEs.\\n    *   **Error Handling:** All invalid inputs result in graceful error messages, not system crashes.\\n    *   **Data Drift:** Model performance degradation below a threshold (e.g., 10% increase in MAE) should trigger alerts.\\n\\n*   **Pass/Fail Criteria:**\\n    *   **PASS:** All specified quantitative metrics are met, all qualitative checks are satisfactory, and no critical defects are found.\\n    *   **FAIL:** Any quantitative metric fails to meet its threshold, critical business logic or safety constraints are violated, or major defects preventing model usage are identified.\\n\\n### 9. Roles & Responsibilities\\n\\n*   **Lead Data Scientist (You):** Overall test plan ownership, defining test strategy, reviewing test results, approving model for deployment.\\n*   **Data Scientists:** Develop unit tests, implement evaluation metrics, analyze model performance, conduct residual analysis, investigate model-related defects.\\n*   **Data Engineers:** Ensure data quality and availability, support data validation tests, investigate data pipeline-related defects.\\n*   **QA Engineer(s) (if applicable):** Execute integration tests, manage test cases, log defects, ensure test environment stability.\\n*   **Business Analysts/SMEs:** Provide input on business rules and constraints, validate model recommendations for operational feasibility and safety.\\n*   **Project Manager:** Facilitate communication, track progress, manage resources, approve timelines.\\n\\n### 10. Schedule & Timeline (High-Level)\\n\\n*   **Phase 1: Planning & Setup (1 week)**\\n    *   Define test data requirements, environment setup.\\n    *   Finalize test plan.\\n*   **Phase 2: Unit & Data Validation Testing (2 weeks)**\\n    *   Individual component testing, automated data quality checks.\\n*   **Phase 3: Model Training & Evaluation Testing (2 weeks)**\\n    *   Performance metric calculation, residual analysis, robustness testing.\\n*   **Phase 4: Business Logic & Integration Testing (2 weeks)**\\n    *   SME review, constraint validation, API testing.\\n*   **Phase 5: User Acceptance Testing (UAT) & Sign-off (1 week)**\\n    *   Final stakeholder review and approval.\\n\\n### 11. Reporting & Defect Management\\n\\n*   **Test Report:** A summary document will be generated at the end of each major testing phase, detailing test coverage, executed test cases, results, identified defects, and overall model readiness.\\n*   **Defect Tracking:** All identified issues, bugs, or discrepancies will be logged in a centralized defect tracking system (e.g., JIRA). Each defect will include:\\n    *   Description\\n    *   Steps to reproduce\\n    *   Expected vs. Actual results\\n    *   Severity (Critical, Major, Minor)\\n    *   Priority (High, Medium, Low)\\n    *   Assigned owner\\n*   **Communication:** Regular status meetings will be held with stakeholders to review progress, discuss blockers, and prioritize defect resolution.\\n\\n### 12. Sign-off\\n\\nBy signing below, the undersigned parties acknowledge their review and approval of this Test Plan.\\n\\n---\\n**Lead Data Scientist:**\\nName: [Your Name]\\nSignature: _________________________\\nDate: _________________________\\n\\n**Project Manager:**\\nName: [PM Name]\\nSignature: _________________________\\nDate: _________________________\\n\\n**Business Stakeholder / SME:**\\nName: [SME Name]\\nSignature: _________________________\\nDate: _________________________\\n\\n---', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c0f33-976d-7a41-b275-b5a900291c10-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 26, 'output_tokens': 5606, 'total_tokens': 5632, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1529}})]}}\n",
      "{'reviewer': {'review_feedback': 'This test plan is remarkably comprehensive and well-structured. It demonstrates a deep understanding of the complexities involved in deploying a machine learning model, especially one with safety-critical implications like cargo weight optimization. The detailed test cases, particularly those for business logic and domain-specific constraints, are excellent.\\n\\nHowever, being critical, here are two improvements focusing on \"missing tools\" and \"unrealistic timelines\":\\n\\n---\\n\\n### 1. Unrealistic Timelines for a Critical, Comprehensive Project\\n\\n*   **Observation:** The proposed 8-week timeline (2 months) for a project of this complexity, criticality (safety implications), and the sheer breadth of detailed testing outlined is highly optimistic. Each phase, particularly those involving iterative feedback from Subject Matter Experts (SMEs) or deep analytical work (robustness tests, residual analysis), can uncover significant issues that require additional development, re-testing, and bug fixing. For example:\\n    *   **Phase 2: Unit & Data Validation Testing (2 weeks):** While unit tests can be quick, robust and automated data validation often uncovers data quality issues that require significant time to investigate and resolve with the Data Engineering team.\\n    *   **Phase 4: Business Logic & Integration Testing (2 weeks):** This is arguably the most critical phase given the safety and operational constraints. Thorough validation with SMEs often involves multiple rounds of feedback, clarification, and model adjustments, which can easily extend beyond two weeks.\\n    *   **Phase 5: User Acceptance Testing (UAT) & Sign-off (1 week):** UAT, especially for a new system impacting core operations, frequently reveals new requirements or edge cases, leading to further development cycles. A single week for final sign-off is very tight if any issues arise.\\n*   **Impact:** Underestimating timelines can lead to rushed testing, missed defects, stakeholder frustration, and ultimately, a less robust or even unsafe deployment.\\n*   **Improvement:** Revise the \"Schedule & Timeline\" section to be more realistic and flexible. This could involve:\\n    1.  **Adding explicit buffer/contingency:** Acknowledge that the timeline is an estimate and includes dedicated time (e.g., 20-30% buffer) for defect resolution, re-testing, and potential scope adjustments.\\n    2.  **Emphasizing iteration:** Clarify that phases may not be strictly sequential and that iteration (e.g., returning to development after UAT feedback) is an expected part of the process.\\n    3.  **Potentially extending specific phases:** Re-evaluate if 2 weeks for \"Business Logic & Integration Testing\" or 1 week for \"UAT & Sign-off\" is sufficient given the safety-critical nature and the need for thorough SME validation.\\n    4.  **Breaking down complex phases:** For instance, \"Business Logic & Integration Testing\" could be split into \"Business Logic Validation\" (with SMEs) and \"API Integration Testing\" to better estimate and manage the distinct efforts.\\n\\n---\\n\\n### 2. Missing Dedicated Tools for Automated Data Validation and Continuous ML Monitoring\\n\\n*   **Observation:** While the plan mentions MLflow/DVC for model tracking/versioning, Docker for containerization, and Python libraries, it lacks explicit mention of dedicated frameworks or tools for *automating* the extensive data validation, continuous model testing, and crucial post-deployment monitoring. The plan outlines excellent test cases (e.g., schema validation, range checks, data drift simulation, shadow/A/B testing), but doesn\\'t specify the *mechanisms* or *tools* to run these automatically and continuously, especially in a CI/CD context or for production oversight. The \"Production Data Subset (for Shadow/A/B Testing)\" is mentioned, but the tools to *implement and analyze* such testing in a live environment are absent.\\n*   **Impact:** Without dedicated tools for automation and continuous monitoring, data quality issues might go undetected, model performance could degrade silently in production, and the effort required for manual validation would be unsustainable. This directly undermines the goals of \"Robust,\" \"Reliable,\" and \"Valuable\" over the long term.\\n*   **Improvement:** Enhance the \"Test Environment\" and \"Test Cases\" sections by incorporating:\\n    1.  **Automated Data Validation Libraries:** Explicitly list tools like **`Great Expectations`** or **`Pandera`** to programmatically define, validate, and enforce data quality expectations within the data ingestion and feature engineering pipelines. This ensures that \"Data Validation Tests\" (7.2) are not just one-off checks but continuous safeguards.\\n    2.  **ML Monitoring Platforms:** Introduce tools for continuous model performance monitoring, data drift detection, and concept drift detection in production. Examples include **Evidently AI**, **Arize**, **Sagemaker Model Monitor (for AWS)**, or more general observability platforms like Datadog/Prometheus integrated with custom ML metrics. This is critical for ensuring the model remains \"Accurate,\" \"Reliable,\" and \"Valuable\" post-deployment, especially since \"data drift simulation\" and \"shadow/A/B testing\" are already in scope for testing. These tools provide the necessary infrastructure to manage the \"Production Data Subset\" mentioned in the test data section.\\n\\n---', 'messages': [AIMessage(content='This test plan is remarkably comprehensive and well-structured. It demonstrates a deep understanding of the complexities involved in deploying a machine learning model, especially one with safety-critical implications like cargo weight optimization. The detailed test cases, particularly those for business logic and domain-specific constraints, are excellent.\\n\\nHowever, being critical, here are two improvements focusing on \"missing tools\" and \"unrealistic timelines\":\\n\\n---\\n\\n### 1. Unrealistic Timelines for a Critical, Comprehensive Project\\n\\n*   **Observation:** The proposed 8-week timeline (2 months) for a project of this complexity, criticality (safety implications), and the sheer breadth of detailed testing outlined is highly optimistic. Each phase, particularly those involving iterative feedback from Subject Matter Experts (SMEs) or deep analytical work (robustness tests, residual analysis), can uncover significant issues that require additional development, re-testing, and bug fixing. For example:\\n    *   **Phase 2: Unit & Data Validation Testing (2 weeks):** While unit tests can be quick, robust and automated data validation often uncovers data quality issues that require significant time to investigate and resolve with the Data Engineering team.\\n    *   **Phase 4: Business Logic & Integration Testing (2 weeks):** This is arguably the most critical phase given the safety and operational constraints. Thorough validation with SMEs often involves multiple rounds of feedback, clarification, and model adjustments, which can easily extend beyond two weeks.\\n    *   **Phase 5: User Acceptance Testing (UAT) & Sign-off (1 week):** UAT, especially for a new system impacting core operations, frequently reveals new requirements or edge cases, leading to further development cycles. A single week for final sign-off is very tight if any issues arise.\\n*   **Impact:** Underestimating timelines can lead to rushed testing, missed defects, stakeholder frustration, and ultimately, a less robust or even unsafe deployment.\\n*   **Improvement:** Revise the \"Schedule & Timeline\" section to be more realistic and flexible. This could involve:\\n    1.  **Adding explicit buffer/contingency:** Acknowledge that the timeline is an estimate and includes dedicated time (e.g., 20-30% buffer) for defect resolution, re-testing, and potential scope adjustments.\\n    2.  **Emphasizing iteration:** Clarify that phases may not be strictly sequential and that iteration (e.g., returning to development after UAT feedback) is an expected part of the process.\\n    3.  **Potentially extending specific phases:** Re-evaluate if 2 weeks for \"Business Logic & Integration Testing\" or 1 week for \"UAT & Sign-off\" is sufficient given the safety-critical nature and the need for thorough SME validation.\\n    4.  **Breaking down complex phases:** For instance, \"Business Logic & Integration Testing\" could be split into \"Business Logic Validation\" (with SMEs) and \"API Integration Testing\" to better estimate and manage the distinct efforts.\\n\\n---\\n\\n### 2. Missing Dedicated Tools for Automated Data Validation and Continuous ML Monitoring\\n\\n*   **Observation:** While the plan mentions MLflow/DVC for model tracking/versioning, Docker for containerization, and Python libraries, it lacks explicit mention of dedicated frameworks or tools for *automating* the extensive data validation, continuous model testing, and crucial post-deployment monitoring. The plan outlines excellent test cases (e.g., schema validation, range checks, data drift simulation, shadow/A/B testing), but doesn\\'t specify the *mechanisms* or *tools* to run these automatically and continuously, especially in a CI/CD context or for production oversight. The \"Production Data Subset (for Shadow/A/B Testing)\" is mentioned, but the tools to *implement and analyze* such testing in a live environment are absent.\\n*   **Impact:** Without dedicated tools for automation and continuous monitoring, data quality issues might go undetected, model performance could degrade silently in production, and the effort required for manual validation would be unsustainable. This directly undermines the goals of \"Robust,\" \"Reliable,\" and \"Valuable\" over the long term.\\n*   **Improvement:** Enhance the \"Test Environment\" and \"Test Cases\" sections by incorporating:\\n    1.  **Automated Data Validation Libraries:** Explicitly list tools like **`Great Expectations`** or **`Pandera`** to programmatically define, validate, and enforce data quality expectations within the data ingestion and feature engineering pipelines. This ensures that \"Data Validation Tests\" (7.2) are not just one-off checks but continuous safeguards.\\n    2.  **ML Monitoring Platforms:** Introduce tools for continuous model performance monitoring, data drift detection, and concept drift detection in production. Examples include **Evidently AI**, **Arize**, **Sagemaker Model Monitor (for AWS)**, or more general observability platforms like Datadog/Prometheus integrated with custom ML metrics. This is critical for ensuring the model remains \"Accurate,\" \"Reliable,\" and \"Valuable\" post-deployment, especially since \"data drift simulation\" and \"shadow/A/B testing\" are already in scope for testing. These tools provide the necessary infrastructure to manage the \"Production Data Subset\" mentioned in the test data section.\\n\\n---', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c0f34-0fbb-7992-bb3f-c823b1254e41-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 4096, 'output_tokens': 3503, 'total_tokens': 7599, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 2403}})]}}\n",
      "{'__interrupt__': ()}\n"
     ]
    }
   ],
   "source": [
    "# Create a unique thread ID for this conversation\n",
    "config = {\"configurable\": {\"thread_id\": \"EPAM_Interview_Demo_1\"}}\n",
    "\n",
    "initial_input = {\"project_idea\": \"Airlines Cargo Weight Optimization using Linear Regression\", \"messages\": []}\n",
    "\n",
    "# The agent will run until it hits the 'human_approval' node and then STOP.\n",
    "for event in app.stream(initial_input, config):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d711db96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Finalized!\n"
     ]
    }
   ],
   "source": [
    "# This acts as the 'Human' saying 'Yes, I approve'\n",
    "app.invoke(None, config) \n",
    "print(\"Project Finalized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814aada5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
